# lite_model_training/model_config_full.yaml (Tailored for Colab T4 GPU - 16GB VRAM)

block_size: 256
n_layer: 6
n_head: 8
n_embd: 512

max_segment_types: 512 
max_intra_line_positions: 96 

dropout: 0.1         
bias: True           

# Training specific
batch_size: 16       
learning_rate: 0.0003 
epochs: 500           # <--- INCREASED SIGNIFICANTLY
grad_accumulation_steps: 4 
                           
eval_interval_steps: 200   # Evaluate more frequently relative to total steps
                           # If 1 epoch is very fast, 200 global steps might be many epochs.
                           # Consider evaluation every X epochs instead, or adjust this number.
                           # Let's calculate: If 1 epoch = (70 songs / 16 batch_size) ~= 4-5 batches
                           # Effective batches per epoch = 4-5 / 4 (grad_acc) ~= 1 global step per epoch.
                           # This means eval_interval_steps: 200 is effectively eval every ~200 epochs.
                           # This is too infrequent.
                           # Let's aim for evaluation roughly every 5-10 actual epochs.
                           # If 1 effective epoch = 1 global step:
                           # eval_interval_steps: 10 (eval every 10 effective epochs)

eval_interval_steps: 20 # <--- ADJUSTED: Evaluate more often if epochs are fast
                         # This means every 20 global optimizer steps.
                         # If 1 epoch takes ~1 global step (as calculated above), this is ~every 20 epochs.

save_interval_steps: 100 # <--- ADJUSTED: Save more often
                          # ~ every 100 epochs if 1 epoch = 1 global step.

weight_decay: 0.01
seed: 42

dataloader_num_workers: 2
use_amp: True